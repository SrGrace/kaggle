{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.text import *","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\ndef remove_stopwords(input_text):\n    stopwords_list = stopwords.words('english')\n    # Some words which might indicate a certain sentiment are kept via a whitelist\n    whitelist = [\"n't\", \"not\", \"no\"]\n    words = input_text.split()\n    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n    return \" \".join(clean_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntrain['text'] = train['text'].str.replace(\"[^a-zA-Z]\", \" \")\ntrain.text = train.text.apply(remove_stopwords)\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(\"sentiment\",data=train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nCounter(train.sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reps0 = [5 if val == 0 else 1 for val in train.sentiment]\ntrain = train.loc[np.repeat(train.index.values, reps0)]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reps1 = [4 if val == 1 else 1 for val in train.sentiment]\ntrain = train.loc[np.repeat(train.index.values, reps1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.reset_index(inplace=True, drop=True)\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Counter(train.sentiment))\nsns.countplot(\"sentiment\",data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\ntest['text'] = test['text'].str.replace(\"[^a-zA-Z]\", \" \")\ntest.text = test.text.apply(remove_stopwords)\ntest_id = test['unique_hash']\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create databunch\ndata = (TextList.from_df(train, cols='text')\n                .split_by_rand_pct(0.2)\n                .label_for_lm()  \n                .databunch(bs=48))\ndata.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fit the deep learning model with domain specific data\n**","metadata":{}},{"cell_type":"code","source":"learn = language_model_learner(data, AWD_LSTM, drop_mult=0.3)\n\n# select the appropriate learning rate\nlearn.lr_find()\n\n# we typically find the point where the slope is steepest\nlearn.recorder.plot()\n\n# Fit the model based on selected learning rate\nlearn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7))\n\nlearn.unfreeze()\nlearn.fit_one_cycle(5, slice(2e-3/100, 2e-3))\n\n# Save the encoder for use in classification\nlearn.save_encoder('fine_tuned_enc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Re-fit model with classification label**","metadata":{}},{"cell_type":"code","source":"label_cols = ['sentiment']\n\ntest_datalist = TextList.from_df(test, cols='text', vocab=data.vocab)\n\ndata_clas = (TextList.from_df(train, cols='text', vocab=data.vocab)\n             .split_by_rand_pct(0.2)\n             .label_from_df(cols= label_cols, classes=[0, 1, 2])\n             .add_test(test_datalist)\n             .databunch(bs=32))\n\ndata_clas.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlearn_classifier = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n\n# load the encoder saved  \nlearn_classifier.load_encoder('fine_tuned_enc')\nlearn_classifier.freeze()\n\n# select the appropriate learning rate\nlearn_classifier.lr_find()\n\n# we typically find the point where the slope is steepest\nlearn_classifier.recorder.plot()\n\n# Fit the model based on selected learning rate\nlearn_classifier.fit_one_cycle(5, 1e-2, moms=(0.8,0.7))\n\nlearn_classifier.unfreeze()\nlearn_classifier.fit_one_cycle(5, slice(2e-3/100, 2e-3))\n\nlearn_classifier.show_results()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Get predictions**","metadata":{}},{"cell_type":"code","source":"\npreds, target = learn_classifier.get_preds(DatasetType.Test, ordered=True)\npredictions = np.argmax(preds, axis=1) \n\nsubmission = pd.DataFrame({'unique_hash': test_id})\nsubmission = pd.concat([submission, pd.DataFrame(predictions.numpy(), columns = label_cols)], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To download the submission file without Commiting the kernel.\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"subm.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}